# BelarusianGLUE: Towards a Natural Language Understanding Benchmark for Belarusian

This is the evaluation code folder accompanying the paper [BelarusianGLUE: Towards a Natural Language Understanding Benchmark for Belarusian](https://aclanthology.org/2025.acl-long.25/) by Maksim Aparovich, Volha Harytskaya, Vladislav Poritski, Oksana Volchek, and Pavel Smrz (ACL 2025).

This repository contains code for models evaluation and fine-tuning, and the benchmark datasets are available for download [**here**](https://hf.co/datasets/maaxap/BelarusianGLUE).


Repository structure:
- `human` -- code related to the human baseline performance evaluation (section 4.1);
- `bert` -- code related to BERT models evaluation (section 4.2);
- `llm` -- code related to LLM evaluation (section 4.3).
